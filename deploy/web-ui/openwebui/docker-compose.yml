version: "3.8"

# Portal — Open WebUI Stack
# Usage: docker compose up -d
# Requires: Ollama running on host at :11434
#           Portal router running on host at :8000
#           Portal WebInterface running on host at :8081

services:
  # -----------------------------------------------------------------------
  # Caddy — reverse proxy and static file server
  # -----------------------------------------------------------------------
  caddy:
    image: caddy:2.11-alpine
    container_name: portal-caddy
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - ${AI_OUTPUT_DIR:-${HOME}/AI_Output}:/srv/ai-output:ro
    environment:
      - DOCKER_HOST_IP=${DOCKER_HOST_IP:-host.docker.internal}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      start_period: 15s
      retries: 3
    depends_on:
      open-webui:
        condition: service_healthy

  # -----------------------------------------------------------------------
  # Open WebUI — browser chat UI
  # -----------------------------------------------------------------------
  open-webui:
    image: ghcr.io/open-webui/open-webui:v0.8.5
    container_name: portal-open-webui
    restart: unless-stopped
    volumes:
      - open-webui-data:/app/backend/data
    environment:
      # Point to Portal WebInterface as the OpenAI-compatible backend
      - OPENAI_API_BASE_URL=http://portal-core:8081/v1
      - OPENAI_API_KEY=portal
      # Also expose direct Ollama access for model management
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-portal-secret-change-me}
      - WEBUI_AUTH=${WEBUI_AUTH:-false}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3
    depends_on:
      portal-core:
        condition: service_healthy

  # -----------------------------------------------------------------------
  # Portal Core — WebInterface (OpenAI-compatible endpoint)
  # -----------------------------------------------------------------------
  portal-core:
    build:
      context: ../../../
      dockerfile: Dockerfile
    container_name: portal-core
    restart: unless-stopped
    ports:
      - "127.0.0.1:8081:8081"
    volumes:
      - ${HOME}/.portal:/home/portal/.portal
      - ${HOME}/Projects:/mnt/projects:ro
    environment:
      - COMPUTE_BACKEND=${COMPUTE_BACKEND:-mps}
      - OLLAMA_HOST=http://host.docker.internal:11434
      - ROUTER_PORT=8000
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-http://localhost:8080,http://127.0.0.1:8080}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3

volumes:
  open-webui-data:
